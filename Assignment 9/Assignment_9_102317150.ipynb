{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "57f9485b-91eb-405e-80e7-17c955786506",
      "cell_type": "code",
      "source": "import nltk\nimport string\nfrom nltk.corpus import stopwords,wordnet\nfrom nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.probability import FreqDist\nimport matplotlib.pyplot as plt\nimport nltk\nimport re\nnltk.download('punkt_tab')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aace4ef5-5aed-481e-87e9-cedd536a4ab1",
      "cell_type": "code",
      "source": "# Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n# technology, food, books, etc.).\n# 1. Convert text to lowercase and remove punctuaƟon.\n# 2. Tokenize the text into words and sentences.\n# 3. Remove stopwords (using NLTK's stopwords list).\n# 4. Display word frequency distribuƟon (excluding stopwords)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "174f4e38-29cb-47fd-8e9c-7b8aedd4f642",
      "cell_type": "code",
      "source": "paragraph = \"\"\"\nOne of my favorite topics is Formula 1 racing, a perfect blend of technology, strategy, and raw speed. \nWatching drivers push their machines to the limit at over 300 km/h while navigating tight corners and unpredictable\nweather is nothing short of thrilling. What makes F1 even more fascinating is the intricate engineering behind each \ncar—teams invest millions in aerodynamics, tire management, and hybrid power units. Beyond the racing, there's a \ntactical chess match happening between pit walls, where every decision on pit stops and tire choices can make or \nbreak a race. It's not just about who drives the fastest, but who can outthink and outlast the competition. \nFormula 1 is more than a sport—it's a high-stakes drama that unfolds at breakneck speed.\n\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "3f69ea97-2d46-48f1-b5bb-0be266410db2",
      "cell_type": "code",
      "source": "lower_nopunct = paragraph.lower().translate(str.maketrans('', '', string.punctuation))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "ea38070d-5f7f-448b-87a5-1ef617673435",
      "cell_type": "code",
      "source": "sentences = sent_tokenize(lower_nopunct)\nwords = word_tokenize(lower_nopunct)\n\nprint(\"Tokenized Sentences:\\n\", sentences)\nprint(\"\\nTokenized Words:\\n\", words)\n\nstop_words = set(stopwords.words('english'))\nfiltered_words = [word for word in words if word not in stop_words]\n\nprint(\"\\nFiltered Words (Stopwords Removed):\\n\", filtered_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d44518a-a171-4ff9-9d06-4e8e5240913e",
      "cell_type": "code",
      "source": "fdist = FreqDist(filtered_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9ed4d44c-2348-4124-a37c-6244cb3e09e4",
      "cell_type": "code",
      "source": "print(\"\\n10 Top Most Common Words:\")\nfor word, freq in fdist.most_common(10):\n    print(f\"{word}: {freq}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b3536bae-5751-4bd6-aaa8-e342eda7abea",
      "cell_type": "code",
      "source": "# Plot\nfdist.plot(10, title=\"Top 10 Word Frequencies (Excluding Stopwords)\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a390b7f5-fbc2-4e13-915e-0aac061a9415",
      "cell_type": "code",
      "source": "# Q2: Stemming and LemmaƟzaƟon\n# 1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n# 2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n# 3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n# 4. Compare and display results of both techniques. ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8afb706f-bf87-4b90-b2dd-780325f3b8a9",
      "cell_type": "code",
      "source": "text = paragraph.lower().translate(str.maketrans('', '', string.punctuation))\nwords = word_tokenize(text)\nfiltered_words = [word for word in words if word not in stopwords.words('english')]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f4f042da-6909-4e49-bf84-474daff0316e",
      "cell_type": "code",
      "source": "porter = PorterStemmer()\nlancaster = LancasterStemmer()\nlemmatizer = WordNetLemmatizer()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "58571d90-0392-4ca8-93e1-5a41412b0f93",
      "cell_type": "code",
      "source": "print(f\"{'Original':<15}{'Porter Stem':<15}{'Lancaster Stem':<20}{'Lemmatized':<15}\")\nprint(\"-\" * 65)\nfor word in filtered_words:\n    print(f\"{word:<15}{porter.stem(word):<15}{lancaster.stem(word):<20}{lemmatizer.lemmatize(word):<15}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7d7bc270-2bfc-43a3-b2bc-561255e12aea",
      "cell_type": "code",
      "source": "# Q3. Regular Expressions and Text Spliƫng\n# 1. Take their original text from Question 1.\n# 2. Use regular expressions to:\n# a. Extract all words with more than 5 leƩers.\n# b. Extract all numbers (if any exist in their text).\n# c. Extract all capitalized words.\n# 3. Use text spliƫng techniques to:\n# a. Split the text into words containing only alphabets (removing digits and special\n# characters).\n# b. Extract words starƟng with a vowel.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de18c0df-a2e5-4449-aa0f-a99fc13c3116",
      "cell_type": "code",
      "source": "words_more_than_5 = re.findall(r'\\b[a-zA-Z]{6,}\\b', paragraph)\nprint(\"Words with more than 5 letters:\\n\", words_more_than_5)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "370d95ff-e915-41fa-b0cc-514de2abd3ad",
      "cell_type": "code",
      "source": "numbers = re.findall(r'\\b\\d+\\b', paragraph)\nprint(\"\\nNumbers in text:\\n\", numbers if numbers else \"No numbers found.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "eac38afd-6c8e-49de-9c3e-a3887243164d",
      "cell_type": "code",
      "source": "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', paragraph)\nprint(\"\\nCapitalized words:\\n\", capitalized_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1448d5a7-b495-4dc0-99bf-610084782224",
      "cell_type": "code",
      "source": "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', paragraph)\nprint(\"\\nAlphabetic words:\\n\", alphabetic_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "62d4e96d-0988-423e-ae01-069e498cb0b3",
      "cell_type": "code",
      "source": "vowel_words = re.findall(r'\\b[aeiouAEIOU][a-zA-Z]*\\b', paragraph)\nprint(\"\\nWords starting with a vowel:\\n\", vowel_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c88315a2-6110-4e7c-887a-49524e46e805",
      "cell_type": "code",
      "source": "# Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n# 1. Take original text from QuesƟon 1.\n# 2. Write a custom tokenizaƟon funcƟon that:\n# a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n# \"isn't\" should not be split into \"is\" and \"n't\").\n# b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n# a single token).\n# c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n# should remain as is).\n# 3. Use Regex SubsƟtuƟons (re.sub) to:\n# a. Replace email addresses with '<EMAIL>' placeholder.\n# b. Replace URLs with '<URL>' placeholder.\n# c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n# '<PHONE>' placeholder. ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "431aea6d-6839-4fe6-a54e-196a5f4ef4db",
      "cell_type": "code",
      "source": "# Custom Tokenization Function\n\ndef custom_tokenizer(text):\n    # Keep contractions and hyphenated words\n    pattern = r\"\"\"\n        \\b(?:\\d+\\.\\d+|\\d+)\\b                    # Decimal or integer numbers\n        | \\b\\w+(?:-\\w+)*\\b                      # Words, possibly hyphenated\n        | \\b\\w+'\\w+\\b                           # Contractions like isn't, don't\n    \"\"\"\n    tokens = re.findall(pattern, text, re.VERBOSE)\n    return tokens",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ece08452-be01-4072-87af-d1adf25251fd",
      "cell_type": "code",
      "source": "# Regex-based Text Cleaning\n\ndef clean_text(text):\n    # Replace email addresses\n    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n\n    # Replace URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text)\n\n    # Replace phone numbers (international or local)\n    text = re.sub(r'(\\+?\\d{1,3}[\\s-])?\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4}', '<PHONE>', text)\n\n    return text",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6d5c2c65-17a7-4e1f-900a-54bdb772f3c2",
      "cell_type": "code",
      "source": "cleaned_paragraph = clean_text(paragraph)\ntokens = custom_tokenizer(cleaned_paragraph)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5d69e51c-6c6c-4080-9358-eea6183cc6c9",
      "cell_type": "code",
      "source": "print(\"Cleaned Text:\\n\", cleaned_paragraph)\nprint(\"\\nCustom Tokens:\\n\", tokens)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}