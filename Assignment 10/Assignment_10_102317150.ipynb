{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "d9fdf5e2-0b39-4b3c-bd01-4267dba2d644",
      "cell_type": "code",
      "source": "import re\nimport nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom nltk.probability import FreqDist\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d9b58fe-4950-420a-8f36-9b1d75bd7796",
      "cell_type": "raw",
      "source": "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\ntechnology, food, books, etc.).\n1. Convert text to lowercase and remove punctuaƟon using re.\n2. Tokenize the text into words and sentences.\n3. Split using split() and word_tokenize() and compare how Python split and NLTK’s\nword_tokenize() differ.\n4. Remove stopwords (using NLTK's stopwords list).\n5. Display word frequency distribuƟon (excluding stopwords).",
      "metadata": {}
    },
    {
      "id": "bad0c59d-f660-4051-a2a6-ad5419d7335b",
      "cell_type": "code",
      "source": "text = \"\"\"Books have always been my favorite escape, offering a window into worlds far beyond my own. Whether it's the gripping plot twists of a mystery\nnovel or the rich, immersive landscapes of fantasy, each book feels like a new adventure waiting to unfold. I especially enjoy how stories can evoke such\ndeep emotions—joy, sadness, excitement—just through the power of words. Reading also fuels my curiosity and teaches me new perspectives, allowing me to \nunderstand cultures, ideas, and people I may never meet in real life. There’s something deeply comforting about the quiet companionship of a good book. \nIt's like having a universe tucked between two covers, ready to come alive whenever I turn the page.\"\"\"\n\n# Convert to lowercase\ntext_lower = text.lower()\n\n# Remove punctuation\ntext_clean = re.sub(r'[^\\w\\s]', '', text_lower)\n\nprint(text_clean)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ade1f869-8d32-4954-bf16-7a15486b94c1",
      "cell_type": "code",
      "source": "sentences = sent_tokenize(text)\nprint(\"Sentences:\", sentences)\n\nwords = word_tokenize(text_clean)\nprint(\"Words:\", words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9ae26d09-9bbf-4145-8fe5-713d6a6f41f6",
      "cell_type": "code",
      "source": "# Using split()\nsplit_words = text_clean.split()\nprint(\"Split words:\", split_words)\n\n# Using word_tokenize()\ntokenized_words = word_tokenize(text_clean)\nprint(\"Tokenized words:\", tokenized_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "469055a1-a3a6-4af5-aa0c-fc24b801669c",
      "cell_type": "code",
      "source": "stop_words = set(stopwords.words('english'))\n\nfiltered_words = [word for word in tokenized_words if word not in stop_words]\nprint(\"Filtered Words:\", filtered_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "399dcd49-22b7-4630-b141-e6cf8083d65f",
      "cell_type": "code",
      "source": "fdist = FreqDist(filtered_words)\n\nfdist.plot(20, title=\"Word Frequency (without Stopwords)\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8aa99ec1-b61e-47b6-b28e-9076ac147772",
      "cell_type": "raw",
      "source": "Q2. Using the same paragraph from Q1:\n\nExtract all words with only alphabets using re.findall()\nRemove stop words using NLTK’s stopword list\nPerform stemming with PorterStemmer\nPerform lemmaƟzaƟon with WordNetLemmaƟzer\nCompare the stemmed and lemmaƟzed outputs and explain when you’d prefer one over the other.",
      "metadata": {}
    },
    {
      "id": "060a3a4d-19ec-410a-ada4-0263d6e98579",
      "cell_type": "code",
      "source": "text_lower = text.lower()\n\nalphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text_lower)\nprint(\"Alphabetic Words:\", alphabetic_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ee2ae13f-ea1e-45bb-8891-edadfd4b7d41",
      "cell_type": "code",
      "source": "stop_words = set(stopwords.words('english'))\n\nfiltered_words = [word for word in alphabetic_words if word not in stop_words]\nprint(\"Filtered Words:\", filtered_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3393ef38-7672-420c-9a67-c20b6b3c4980",
      "cell_type": "code",
      "source": "porter = PorterStemmer()\n\nstemmed_words = [porter.stem(word) for word in filtered_words]\nprint(\"Stemmed Words:\", stemmed_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c08dd5f2-50c2-4056-8346-683daca317c7",
      "cell_type": "code",
      "source": "lemmatizer = WordNetLemmatizer()\n\nlemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\nprint(\"Lemmatized Words:\", lemmatized_words)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "41525bdd-f933-4f3f-862a-cf19a40cd836",
      "cell_type": "raw",
      "source": "Q3. Choose 3 short texts of your own (e.g., different news headlines, product reviews).\n\nUse CountVectorizer to generate the Bag of Words representaƟon.\nUse TfidfVectorizer to compute TF-IDF scores.\nPrint and interpret the top 3 keywords from each text using TF-IDF",
      "metadata": {}
    },
    {
      "id": "c30c3e82-b1b7-4cdc-8d59-9baf7e754d26",
      "cell_type": "code",
      "source": "texts = [\n    \"AI Technology Set to Revolutionize Healthcare Diagnostics by 2026.\",\n    \"The new wireless earbuds have amazing sound quality and battery life, but the touch controls can be a bit too sensitive.\",\n    \"Just tried the caramel cold brew from the new café downtown—absolute game changer!\"\n]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7b3c8a7e-95a0-4a80-92bd-f3d46963fd9f",
      "cell_type": "code",
      "source": "count_vectorizer = CountVectorizer()\n\ncount_matrix = count_vectorizer.fit_transform(texts)\n\n# Show feature names\nprint(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n\n# Show Bag of Words Matrix\nprint(\"Bag of Words Matrix:\\n\", count_matrix.toarray())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e561b3c0-4996-463f-bcd8-603c372c89e9",
      "cell_type": "code",
      "source": "tfidf_vectorizer = TfidfVectorizer()\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n\n# Get feature names\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n\n# Show TF-IDF Matrix\nprint(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4f4f5c0-0e45-49f8-af60-60a0d41870f1",
      "cell_type": "code",
      "source": "# Loop over each text (row)\nfor i in range(len(texts)):\n    print(f\"\\nText {i+1}: {texts[i]}\")\n\n    # Get the row\n    row = tfidf_matrix[i].toarray()[0]\n\n    # Find indices of top 3 scores\n    top_indices = row.argsort()[-3:][::-1]\n\n    # Print top keywords\n    for idx in top_indices:\n        print(f\"   {feature_names[idx]} (Score: {row[idx]:.3f})\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e586458b-051a-4be1-bda9-dcec7a93c392",
      "cell_type": "raw",
      "source": "Q4. Write 2 short texts (4–6 lines each) describing two different technologies (e.g., AI vs Blockchain).\n\nPreprocess and tokenize both texts.\nCalculate: a. Jaccard Similarity using sets b. Cosine Similarity using TfidfVectorizer + cosine_similarity() c. Analyze which similarity metric gives beƩer insights in your case",
      "metadata": {}
    },
    {
      "id": "b65fcfe5-176a-4a54-81e9-b09f9c7ea9b4",
      "cell_type": "code",
      "source": "text1 = \"\"\"Artificial Intelligence refers to the simulation of human intelligence by machines, especially computer systems. It powers applications like\nvoice assistants, recommendation engines, and autonomous vehicles. AI can analyze massive datasets quickly and make decisions or predictions. Its \nlearning capability allows it to improve over time with exposure to more data.\"\"\"\n\ntext2 = \"\"\"Blockchain is a decentralized and secure technology that stores data in blocks connected in a chain. It enables transparent, tamper-proof \ntransactions without the need for intermediaries. Blockchain powers cryptocurrencies like Bitcoin and supports smart contracts.\"\"\"\n\ndef preprocess(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\n    return tokens\n\ntokens1 = preprocess(text1)\ntokens2 = preprocess(text2)\n\nprint(\"Tokens 1:\", tokens1)\nprint(\"Tokens 2:\", tokens2)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d5040a6-9f3d-46e1-80c8-da214d2ab5b0",
      "cell_type": "code",
      "source": "set1 = set(tokens1)\nset2 = set(tokens2)\n\njaccard_sim = len(set1 & set2) / len(set1 | set2)\nprint(\"Jaccard Similarity:\", round(jaccard_sim, 3))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d75895f7-3349-4651-b9f2-491eb81541c0",
      "cell_type": "raw",
      "source": "Q5. Write a short review for a product or service.\n\nUse TextBlob or VADER to find polarity & subjecƟvity for each review.\nClassify reviews into PosiƟve / NegaƟve / Neutral.\nCreate a word cloud using the wordcloud library for all posiƟve reviews",
      "metadata": {}
    },
    {
      "id": "5697eb45-8e67-4fdd-999f-f5d81e450272",
      "cell_type": "code",
      "source": "review = \"\"\"I recently tried the Kindle Paperwhite, and it's easily one of the best e-readers I've used. The display is crisp and glare-free, making \nit perfect for reading even in direct sunlight. Its battery life is impressive—I only need to charge it once every few weeks. I also love the \nadjustable warm light feature, which makes nighttime reading much more comfortable. Overall, it's a lightweight, user-friendly device that’s perfect \nfor book lovers on the go.\"\"\"\n\nblob = TextBlob(review)\npolarity = blob.sentiment.polarity   # between -1 (negative) and 1 (positive)\nsubjectivity = blob.sentiment.subjectivity  # between 0 (objective) and 1 (subjective)\n\nprint(f\"Polarity: {polarity}\")\nprint(f\"Subjectivity: {subjectivity}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "940c4b0e-3e32-4765-bccc-3d37e53c4c76",
      "cell_type": "code",
      "source": "if polarity > 0:\n    sentiment = \"Positive\"\nelif polarity < 0:\n    sentiment = \"Negative\"\nelse:\n    sentiment = \"Neutral\"\n\nprint(\"Sentiment:\", sentiment)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ed0d556c-b548-4a43-8289-44bea712eb2d",
      "cell_type": "code",
      "source": "positive_reviews = \"\"\"I recently tried out the Ember Temperature Control Smart Mug, and it’s honestly a small luxury that makes a big difference.\"\"\"\n\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "08b3e19e-817b-44b6-b44a-d93d285d7769",
      "cell_type": "raw",
      "source": "Q6. Choose your own paragraph (~100 words) as training data.\n\nTokenize text using Tokenizer() from keras.preprocessing.text\nCreate input sequences and build a simple LSTM or Dense model\nTrain the model and generate 2–3 new lines of text starƟng from any seed word you",
      "metadata": {}
    },
    {
      "id": "792f9b10-3c57-40ed-bcc2-445093ac7a89",
      "cell_type": "code",
      "source": "paragraph = \"\"\"Sure! Here's a paragraph of around 100 words that could be used as training data:\n\nTechnology has transformed the way we live, work, and communicate. From smartphones that keep us connected around the clock to AI-powered tools that\nautomate complex tasks, innovation is constantly reshaping our world. One of the most exciting developments is the rise of smart homes, where \neverything from lights to thermostats can be controlled with a voice command. These advancements not only offer convenience but also improve energy \nefficiency and security. As technology continues to evolve, it challenges us to adapt, learn, and imagine new possibilities for the future—making it a \nfascinating area to explore and understand.\"\"\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2b92fb8-595f-4d2d-ae1a-b2980437a124",
      "cell_type": "code",
      "source": "# Initialize and fit tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([paragraph])\n\n# Create input sequences\nsequences = []\ntotal_words = len(tokenizer.word_index) + 1\n\n# Generate sequences of words\nfor line in paragraph.split('.'):\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        sequences.append(n_gram_sequence)\n\n# Pad sequences\nmax_sequence_len = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n\n# Split data into features and labels\nX = sequences[:, :-1]\ny = sequences[:, -1]\ny = np.array(y)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "96148f42-b322-48f4-8b7d-67ee17f96be5",
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fb1dde09-4de1-494a-8be3-0fe86121f022",
      "cell_type": "code",
      "source": "seed_text = \"Artificial intelligence\"\nnext_words = 5\n\nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict(token_list, verbose=0)\n    predicted_word_index = np.argmax(predicted)\n\n    output_word = ''\n    for word, index in tokenizer.word_index.items():\n        if index == predicted_word_index:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\n\nprint(seed_text)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}